from pyspark.sql import functions as F
udf1 = F.udf(lambda x,y: pgh.encode(x,y,precision=7))
geoCordsSchema.select('lat','long',udf1('lat','long').alias('encodedVal')).show()

from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
import Geohash

def calcualte_the_geohash(lat, long):
return Geohash.encode(lat, long, precision=6)

green_trip_data = spark.read.csv("/Users/hmandadi/Downloads/green_tripdata_2018-01.csv", header=True)
yellow_trip_data = spark.read.csv("/Users/hmandadi/Downloads/yellow_tripdata_2018-01.csv", header=True, mode="DROPMALFORMED")


calcualte_the_geohash_udf = udf(lambda x, y: Geohash.encode(x, y), StringType())
green_trip_data = green_trip_data.withColumn("geo_hash", calcualte_the_geohash_udf(green_trip_data.PULocationID, green_trip_data.DOLocationID))

yellow_trip_data.withColumn("geo_hash", calcualte_the_geohash_udf(yellow_trip_data.PULocationID, yellow_trip_data.DOLocationID))




from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
import csv
from pyspark.sql import functions as F
import Geohash
import pygeohash

reader = csv.reader(open('/Users/hmandadi/Downloads/taxi_zones_2.csv', 'r'))
lat_long_dict = {}
for row in reader:
  lat_long_dict[row[2]] = {'lat': row[1], 'long' : row[0]}

lat_long_dict = spark.sparkContext.broadcast(lat_long_dict)

get_lat_for_location_id = udf(lambda x: lat_long_dict.value[x]['lat'], StringType())
get_long_for_location_id = udf(lambda x: lat_long_dict.value[x]['long'], StringType())
calcualte_the_geohash_udf = udf(lambda x, y: Geohash.encode(float(x), float(y)), StringType())

calcualte_the_geohash_udf = udf(lambda x, y: pygeohash.encode(float(x), float(y)), StringType())

supply_data = spark.read.csv("/Users/hmandadi/Desktop/test_2_driver.csv", header=True, mode="DROPMALFORMED")
supply_data = supply_data.select("PULocationID")
supply_data = supply_data.withColumn("lat", get_lat_for_location_id(supply_data.PULocationID)).withColumn("long", get_long_for_location_id(supply_data.PULocationID)).drop("PULocationID")
supply_data = supply_data.withColumn("geo_hash", calcualte_the_geohash_udf(supply_data.lat, supply_data.long))
supply_data_df = supply_data.groupBy("geo_hash").agg(F.count("geo_hash").alias("geo_hash_supply_count"))

#supply_data = spark.read.csv("/Users/hmandadi/Desktop/test_2_driver.csv", header=True, mode="DROPMALFORMED").select("PULocationID").withColumn("lat", get_lat_for_location_id("PULocationID")).withColumn("long", get_long_for_location_id("PULocationID")).drop("PULocationID").withColumn("geo_hash", calcualte_the_geohash_udf("lat", "long")).groupBy("geo_hash").agg(F.count("geo_hash").alias("geo_hash_supply_count"))

#write supply_data_df as parquet to HDFS
supply_data_df.write.format('parquet').mode('append').save('/Users/hmandadi/Desktop/testing_gb_dir/')

demand_data = spark.read.csv("/Users/hmandadi/Desktop/test_1_demand.csv",header=True, mode="DROPMALFORMED")
demand_data = demand_data.select("PULocationID")
demand_data = demand_data.withColumn("lat", get_lat_for_location_id(demand_data.PULocationID)).withColumn("long", get_long_for_location_id(demand_data.PULocationID)).drop("PULocationID")
demand_data = demand_data.withColumn("geo_hash", calcualte_the_geohash_udf(demand_data.lat, demand_data.long))
demand_data_df = demand_data.groupBy("geo_hash").agg(F.count("geo_hash").alias("geo_hash_demand_count"))

#write demand_data_df as parquet to HDFS
demand_data_df.write.format('parquet').mode('append').save('/Users/hmandadi/Desktop/testing_gb_dir/')

supply_to_demand_ratio = supply_data_df.join(demand_data_df, supply_data_df.geo_hash == demand_data_df.geo_hash).select(supply_data_df.geo_hash, supply_data_df.geo_hash_supply_count/demand_data_df.geo_hash_demand_count)

#modified grouping with geo_hash and time till minutes
#supply_data_df = spark.read.csv("/Users/hmandadi/Desktop/test_1_demand.csv", header=True).select(F.col('lpep_pickup_datetime').cast('timestamp').alias('time'), F.col('PULocationID')).select(F.from_unixtime(unix_timestamp('time', 'yyyy-MM-dd HH:mm:ss'), 'yyyy-MM-dd HH:mm').alias('date'), 'PULocationID').withColumn("lat", get_lat_for_location_id("PULocationID")).withColumn("long", get_long_for_location_id("PULocationID")).drop("PULocationID").withColumn("geo_hash", calcualte_the_geohash_udf("lat", "long")).groupBy(["geo_hash", "date"]).agg(F.count("*").alias("supply_count"))



demand_data_df = spark.read.csv("/Users/hmandadi/Desktop/test_2_driver.csv", header=True).select(F.col('lpep_pickup_datetime').cast('timestamp').alias('time'), F.col('PULocationID')).select(F.from_unixtime(unix_timestamp('time', 'yyyy-MM-dd HH:mm:ss'), 'yyyy-MM-dd HH:mm').alias('date'), 'PULocationID').withColumn("lat", get_lat_for_location_id("PULocationID")).withColumn("long", get_long_for_location_id("PULocationID")).drop("PULocationID").withColumn("geo_hash", calcualte_the_geohash_udf("lat", "long")).groupBy(["geo_hash", "date"]).agg(F.count("*").alias("demand_count"))
